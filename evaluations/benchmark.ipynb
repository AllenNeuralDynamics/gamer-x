{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4af2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f7bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = pd.read_excel(r\"C:\\Users\\sreya.kumar\\Downloads\\gamer_benchmark_6_16.xlsx\")\n",
    "\n",
    "benchmark = benchmark.rename(\n",
    "    columns={\n",
    "        \"output_answer\": \"target_answer\",\n",
    "    }\n",
    ")\n",
    "\n",
    "benchmark[\"predicted_answer\"] = pd.Series(dtype=\"str\")\n",
    "benchmark[\"data_source\"] = pd.Series(dtype=\"str\")\n",
    "benchmark[\"generation_time\"] = pd.Series(dtype=\"float\")\n",
    "benchmark[\"response_evaluation\"] = pd.Series(dtype=\"str\")\n",
    "benchmark[\"response_score\"] = pd.Series(dtype=\"int\")\n",
    "benchmark[\"predicted_python\"] = pd.Series(dtype=\"str\")\n",
    "benchmark[\"predicted_mongodb_query\"] = pd.Series(dtype=\"str\")\n",
    "\n",
    "test_df = benchmark[benchmark['query_type'] !=  \"schema_docs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdaf3f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query_type\n",
       "database    36\n",
       "analysis    23\n",
       "field       23\n",
       "project     21\n",
       "asset       16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"query_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gamer_x.agent import main\n",
    "import time\n",
    "\n",
    "from typing import Annotated, Literal, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "from gamer_x.utils.llms import (\n",
    "    SONNET_4_LLM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(TypedDict):\n",
    "    \"\"\"Relevant material in the retrieved document +\n",
    "    Binary score to check relevance to the question\"\"\"\n",
    "\n",
    "    score: Annotated[\n",
    "        Literal[\"CORRECT\", \"INCORRECT\", \"ERROR\"],\n",
    "        ...,\n",
    "        (\n",
    "            \"Predicted response matched target response, 'correct' or 'incorrect'\"\n",
    "            \"Predicted response is an error message, 'error'\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "evaluator = SONNET_4_LLM.with_structured_output(Evaluator)\n",
    "evaluator_prompt = hub.pull(\"eden19/evaluator\")\n",
    "evaluator_chain = evaluator_prompt | evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def eval():\n",
    "    benchmark = test_df\n",
    "    for index, row in benchmark.iterrows():\n",
    "\n",
    "        response = \"Error occurred\"\n",
    "        time_taken = -1\n",
    "        response_evaluation = \"ERROR\"\n",
    "        response_score = 0\n",
    "\n",
    "        query = row[\"input_question\"]\n",
    "        target_response = row[\"target_answer\"]\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                start = time.time()\n",
    "                answer = await main(query)\n",
    "                end = time.time()\n",
    "                time_taken = end - start\n",
    "                response = answer[\"generation\"]\n",
    "\n",
    "            except Exception as e:\n",
    "                response = f\"Error: {e}\"\n",
    "\n",
    "            benchmark.at[index, \"predicted_answer\"] = response\n",
    "            benchmark.at[index, \"generation_time\"] = time_taken\n",
    "\n",
    "            mongodb_response = answer.get(\"mongodb_query\", \"NA\")\n",
    "            python_response = answer.get(\"python_code\", \"NA\")\n",
    "\n",
    "\n",
    "\n",
    "            response_result = await evaluator_chain.ainvoke(\n",
    "                {\n",
    "                    \"query\": query,\n",
    "                    \"target\": target_response,\n",
    "                    \"predicted\": response,\n",
    "                }\n",
    "            )\n",
    "            response_evaluation = response_result[\"score\"]\n",
    "\n",
    "            response_score = 0\n",
    "\n",
    "            if response_evaluation == \"CORRECT\":\n",
    "                response_score = 1\n",
    "\n",
    "        except Exception as e:\n",
    "            response_score = f\"Error: {e}\"\n",
    "\n",
    "        benchmark.at[index, \"response_evaluation\"] = (\n",
    "            response_evaluation\n",
    "        )\n",
    "        benchmark.at[index, \"response_score\"] = response_score\n",
    "\n",
    "    return benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11960104",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await eval()\n",
    "\n",
    "results.to_csv(\"gamer_2.0_evals_part_1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
